<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>system on 1Feng 的技术文章</title>
    <link>http://1feng.github.io/categories/system/</link>
    <description>Recent content in system on 1Feng 的技术文章</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 16 Jun 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://1feng.github.io/categories/system/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Read-Only 的 linearizability</title>
      <link>http://1feng.github.io/post/2017-06-16-smarter/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-16-smarter/</guid>
      <description>《Paxos Replicated State Machines as the Basis of a High-Performance Data Store》 介绍了使用了paxos算法进行副本同步，这里仅总结如何保证read-only操作的lineari</description>
    </item>
    <item>
      <title>CAP 问题</title>
      <link>http://1feng.github.io/post/2017-06-08-cap/</link>
      <pubDate>Thu, 08 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-08-cap/</guid>
      <description>Introduce 于2002年提出的CAP理论（三选二的方式来评估分布式系统）确实为分布式系统领域的发展提供了指导价值，但是就今天而言，这套理论已经意义微小</description>
    </item>
    <item>
      <title>What is ACID</title>
      <link>http://1feng.github.io/post/2017-06-07-acid/</link>
      <pubDate>Wed, 07 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-07-acid/</guid>
      <description>What Atomicity 描述： 一个事务包含一系列的操作，这一系列的操作都成功，则意味着事务执行成功；一旦执行过程中发生故障(fault)，数据库需要放弃整个事务</description>
    </item>
    <item>
      <title>Unreliable Network</title>
      <link>http://1feng.github.io/post/2017-06-06-unreliable-network/</link>
      <pubDate>Tue, 06 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-06-unreliable-network/</guid>
      <description>Introduce 众所周知TCP是可靠的网络传输协议，但是为什么在分布式系统中又认为网络是不可靠的呢？通常有以下两点： 发送方无法确定接收方已经收到请求 发送方</description>
    </item>
    <item>
      <title>Timing and Order</title>
      <link>http://1feng.github.io/post/2017-06-05-timing-and-order/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-05-timing-and-order/</guid>
      <description>Introduce 分布式环境面临的两个主要的问题就是网络不可靠和时钟不可靠，这里主要总结时钟问题 Physical Clocks 我们日常使用的计算机和服务器的物理时钟都是使用的石英(q</description>
    </item>
    <item>
      <title>Vector clock summary</title>
      <link>http://1feng.github.io/post/2017-06-05-vector-clock/</link>
      <pubDate>Mon, 05 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-05-vector-clock/</guid>
      <description>Happend before 用→来表示hanppend before，对于任意event a, b 有： 如果a和b属于同一个process，并且a comes before b, 则 a → b 如果a是某个p</description>
    </item>
    <item>
      <title>《Time, clocks, and the ordering of events in a distributed system》summary</title>
      <link>http://1feng.github.io/post/2017-06-03-time-clocks-and-the-ordering-of-events-in-a-distributed-system/</link>
      <pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2017-06-03-time-clocks-and-the-ordering-of-events-in-a-distributed-system/</guid>
      <description>Happend before 用→来表示hanppend before，对于任意event a, b 有： 如果a和b属于同一个process，并且a comes before b, 则 a → b 如果a是某个p</description>
    </item>
    <item>
      <title>leveldb 源码笔记之 Read</title>
      <link>http://1feng.github.io/post/2016-09-10-leveldb-read/</link>
      <pubDate>Sat, 10 Sep 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-09-10-leveldb-read/</guid>
      <description>key逻辑分类 根据我们之前文章的描述，leveldb的数据存储可能存在在内存的memtable中，或者磁盘的sstalbe中，但是key的实</description>
    </item>
    <item>
      <title>leveldb 源码笔记之 MVCC &amp;&amp; Manifest</title>
      <link>http://1feng.github.io/post/2016-08-24-mvcc-and-manifest/</link>
      <pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-08-24-mvcc-and-manifest/</guid>
      <description>MVCC 问题: 针对同一条记录，如果读和写在同一时间发生时，reader可能会读取到不一致或者写了一半的数据 常见解决方案 悲观锁： 最简单的方式,即通过</description>
    </item>
    <item>
      <title>leveldb 源码笔记之 sstable</title>
      <link>http://1feng.github.io/post/2016-08-22-sstable-summary/</link>
      <pubDate>Mon, 22 Aug 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-08-22-sstable-summary/</guid>
      <description>sstable 组成细节 如下： sstalbe 生成流程 sstable 生成时机: minor compaction immutable-memtable 中的key/value dump到磁盘，生成sstable major compaction sstable compact（level-n ss</description>
    </item>
    <item>
      <title>Signal with mutex locked or not(译)</title>
      <link>http://1feng.github.io/post/2016-07-20-signal-with-mutex-locked-or-not/</link>
      <pubDate>Wed, 20 Jul 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-07-20-signal-with-mutex-locked-or-not/</guid>
      <description>原文链接 介绍 当我们使用条件变量的时候，总有这样一个问题：到底该在解锁mutex之前进行sinal/broadcast，还是在之后？ 什么时候进</description>
    </item>
    <item>
      <title>Google File System 笔记</title>
      <link>http://1feng.github.io/post/2016-07-08-google-file-system-summary/</link>
      <pubDate>Fri, 08 Jul 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-07-08-google-file-system-summary/</guid>
      <description>创新点 把组件故障当做常态，而不是异常。即，要有完备的监控，错误检测，故障恢复机制 通常文件都是巨大的，数以GB是常态 多数文件的修改是追加新数据</description>
    </item>
    <item>
      <title>浮点数问题探究</title>
      <link>http://1feng.github.io/post/2016-06-27-ieee-floating-point-summary/</link>
      <pubDate>Mon, 27 Jun 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-06-27-ieee-floating-point-summary/</guid>
      <description>问题 最近在使用openresty实现一些业务，业务中设计了一套二进制编码，目前为49bit。真正实现的时候发现lua里不支持(u)int64</description>
    </item>
    <item>
      <title>Linux cpu load</title>
      <link>http://1feng.github.io/post/2016-05-05-linux-cpu-load/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      <guid>http://1feng.github.io/post/2016-05-05-linux-cpu-load/</guid>
      <description>简析 工作一直是在linux环境下，经常通过top，uptime来查看当前机器的负载（load average）, 但是始终对这个概念比较模糊，无</description>
    </item>
  </channel>
</rss>
